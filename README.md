# UGround
This is the official code repository for the project: *Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents*.
- [Homepage](https://osu-nlp-group.github.io/UGround)
- [Paper](https://github.com/OSU-NLP-Group/UGround/blob/gh-pages/static/papers/UGround_paper.pdf)
- [Model Weights](https://huggingface.co/osunlp/UGround)
- Online Demo: Coming Soon

<h3>Release Plans:</h3>

- [x] Model Weights
- [ ] Code
  - [ ] Inference Code of UGround
  - [ ] Offline Experiments
    - [x] Screenspot (along with referring expressions generated by GPT-4/4o)
    - [x] Multimodal-Mind2Web
    - [ ] OmniAct
  - [ ] Online Experiments
    - [ ] Mind2Web-Live
    - [ ] AndroidWorld
- [ ] Data
  - [ ] Data Examples
  - [ ] Data Construction Scripts
  - [ ] Guidance of Open-source Data 
  - [ ] Full Data
- [ ] Online Demo (HF Space)

<h3>Updates</h3>

- 2024/10/07: Preprint is arXived. Demo and code coming soon.

- 2024/08/06: Website is live. The initial manuscript and results are available.