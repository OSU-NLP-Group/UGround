# UGround
This is the official code repository for the project: *Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents*.
<img width="1556" alt="image" src="https://github.com/user-attachments/assets/18c6a9f4-31cc-4817-a252-bfd0dbaf3fd6">
- [ğŸ Homepage](https://osu-nlp-group.github.io/UGround)
- [ğŸ“–Paper](https://arxiv.org/abs/2410.05243)
- [ğŸ˜ŠModel Weights](https://huggingface.co/osunlp/UGround)
- [ğŸ˜ŠLive Demo](https://huggingface.co/spaces/orby-osu/UGround) (Try it out yourself!)

<h3>Release Plans:</h3>

- [x] Model Weights
- [ ] Code
  - [ ] Inference Code of UGround
  - [x] Offline Experiments
    - [x] Screenspot (along with referring expressions generated by GPT-4/4o)
    - [x] Multimodal-Mind2Web
    - [x] OmniAct
  - [ ] Online Experiments
    - [ ] Mind2Web-Live
    - [ ] AndroidWorld
- [ ] Data
  - [ ] Data Examples
  - [ ] Data Construction Scripts
  - [ ] Guidance of Open-source Data 
  - [ ] Full Data
- [x] Online Demo (HF Spaces)

<h3>Updates</h3>

- 2024/10/07: Preprint is arXived. Demo and code coming soon.

- 2024/08/06: Website is live. The initial manuscript and results are available.

## Citation Information

If you find this work useful, please consider starring our repo and citing our papers: 

```
@article{gou2024uground,
        title={Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents},
        author={Boyu Gou and Ruohan Wang and Boyuan Zheng and Yanan Xie and Cheng Chang and Yiheng Shu and Huan Sun and Yu Su},
        journal={arXiv preprint arXiv:2410.05243},
        year={2024},
        url={https://arxiv.org/abs/2410.05243},
      }

@article{zheng2023seeact,
        title={GPT-4V(ision) is a Generalist Web Agent, if Grounded},
        author={Boyuan Zheng and Boyu Gou and Jihyung Kil and Huan Sun and Yu Su},
        journal={arXiv preprint arXiv:2401.01614},
        year={2024},
      }
```
