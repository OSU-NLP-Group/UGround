# UGround (Work In Progress)
This is the official code repository for the project: *Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents*.
- [Homepage](https://osu-nlp-group.github.io/UGround)
- [Paper](https://github.com/OSU-NLP-Group/UGround/blob/gh-pages/static/papers/UGround_paper.pdf)
- [Model Weights](https://huggingface.co/osunlp/UGround)
- Online Demo: Coming Soon

<h3>Release Plans:</h3>

- [x] Model Weights
- [ ] Code
  - [ ] Inference Code of UGround
  - [ ] Offline Experiments
    - [x] Screenspot (along with referring expressions generated by GPT-4/4o)
    - [x] Mind2Web
    - [] OmniAct
  - [ ] Online Experiments
    - [ ] Mind2Web-Live
    - [ ] AndroidWorld
- [ ] Data
  - [ ] Data Examples
  - [ ] Data Construction Scripts
  - [ ] Guidance of Open-source Data 
  - [ ] Full Data
- [ ] Online Demo (HF Space)

<h3>Updates</h3>

- 2024/8/17: Crawler mode added!

- 2024/7/9: Support SoM (Set-of-Mark) grounding strategy!

- 2024/5/18: Support for Gemini and LLaVA!

- 2024/5/1: SeeAct has been accepted to ICML'24!

- 2024/4/28: Released [SeeAct Python Package](https://pypi.org/project/seeact/#history), with many updates and many features on the way. Have a try with `pip install seeact`

- 2024/3/18: [Multimodal-Mind2Web](https://huggingface.co/datasets/osunlp/Multimodal-Mind2Web) dataset released. We have paired each HTML document with the corresponding webpage screenshot image and saved the trouble of downloading [Mind2Web Raw Dump](https://github.com/OSU-NLP-Group/Mind2Web?tab=readme-ov-file#raw-dump-with-full-traces-and-snapshots).
