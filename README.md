# UGround
This is the official code repository for the project: *Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents*.
- [ğŸ Homepage](https://osu-nlp-group.github.io/UGround)
- [ğŸ“–Paper](https://arxiv.org/abs/2410.05243)
- [ğŸ˜ŠModel Weights](https://huggingface.co/osunlp/UGround)
- [ğŸ˜ŠOnline Demo](https://huggingface.co/spaces/orby-osu/UGround)
![image](https://github.com/user-attachments/assets/d91c4e0b-2425-46c9-b193-e0f701ad160b)
<h3>Release Plans:</h3>

- [x] Model Weights
- [ ] Code
  - [ ] Inference Code of UGround
  - [x] Offline Experiments
    - [x] Screenspot (along with referring expressions generated by GPT-4/4o)
    - [x] Multimodal-Mind2Web
    - [x] OmniAct
  - [ ] Online Experiments
    - [ ] Mind2Web-Live
    - [ ] AndroidWorld
- [ ] Data
  - [ ] Data Examples
  - [ ] Data Construction Scripts
  - [ ] Guidance of Open-source Data 
  - [ ] Full Data
- [x] Online Demo (HF Spaces)

<h3>Updates</h3>

- 2024/10/07: Preprint is arXived. Demo and code coming soon.

- 2024/08/06: Website is live. The initial manuscript and results are available.
